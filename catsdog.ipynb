{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# 1. Download & Extract Dataset\n",
        "# ========================\n",
        "# The following lines are commented out because they require Kaggle authentication\n",
        "# and the 'kaggle.json' file was not found.\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle kernels pull prtksrmwr/cats-v-dogs-classifiction-using-cnn\n",
        "\n",
        "# The following lines are commented out because the zip file was not found.\n",
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')\n",
        "# zip_ref.extractall('/content')\n",
        "# zip_ref.close()\n",
        "\n",
        "# TODO: Add code here to load your dataset.\n",
        "# You can upload the dataset manually or use alternative methods to download it.\n",
        "# For example, if you have the dataset in a zip file named 'dogs-vs-cats.zip'\n",
        "# uploaded to your Colab environment, you can uncomment the zipfile lines above\n",
        "# and adjust the path if necessary.\n",
        "# If your data is already extracted in '/content/train' and '/content/test',\n",
        "# you can proceed to the Data Preprocessing section.\n",
        "\n",
        "\n",
        "# ========================\n",
        "# 2. Import Libraries\n",
        "# ========================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ========================\n",
        "# 3. Data Preprocessing\n",
        "# ========================\n",
        "# Ensure your training and testing data are in the specified directories.\n",
        "# If you have obtained the dataset and extracted it, these paths should be correct.\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/train',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=128,  # Batch size adjust kar sakte ho 64 bhi\n",
        "    image_size=(128,128),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/test',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    batch_size=128,\n",
        "    image_size=(128,128),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Normalize images\n",
        "def process(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "validation_ds = validation_ds.map(process)\n",
        "\n",
        "# ========================\n",
        "# 4. Create CNN Model\n",
        "# ========================\n",
        "model = Sequential([\n",
        "    Conv2D(32,(3,3),activation='relu',padding='same',input_shape=(128,128,3)),\n",
        "    MaxPool2D((2,2)),\n",
        "\n",
        "    Conv2D(64,(3,3),activation='relu',padding='same'),\n",
        "    MaxPool2D((2,2)),\n",
        "\n",
        "    Conv2D(128,(3,3),activation='relu',padding='same'),\n",
        "    MaxPool2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.6),  # dropout slightly increased\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ========================\n",
        "# 5. Compile Model\n",
        "# ========================\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.0005)  # reduced learning rate\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 6. Callbacks\n",
        "# ========================\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=4,  # slightly increased patience\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 7. Train Model on Full Dataset\n",
        "# ========================\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    validation_data=validation_ds,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# 8. Plot Training History\n",
        "# ========================\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# 1. Read & convert to RGB\n",
        "# Make sure '/content/16.webp' exists or replace with your test image path\n",
        "test_image = cv2.imread('/content/16.webp')\n",
        "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(test_image)\n",
        "\n",
        "# 2. Resize\n",
        "test_image = cv2.resize(test_image, (128,128))\n",
        "\n",
        "# 3. Normalize\n",
        "test_input = test_image.astype('float32') / 255.0\n",
        "test_input = np.expand_dims(test_input, axis=0)\n",
        "\n",
        "# 4. Predict\n",
        "prediction = model.predict(test_input)\n",
        "print(\"Raw prediction:\", prediction[0][0])\n",
        "\n",
        "# 5. Human readable\n",
        "if prediction[0][0] < 0.5:\n",
        "    print(\"Prediction â†’ cat ðŸ±\")\n",
        "else:\n",
        "    print(\"Prediction â†’ dog ðŸ¶ \")"
      ],
      "metadata": {
        "id": "YAnfP1_2umdZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "6e1cf9a4-5b5b-4450-8061-3acb0c14c3ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Could not find directory /content/train",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1320366677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Ensure your training and testing data are in the specified directories.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# If you have obtained the dataset and extracted it, these paths should be correct.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m train_ds = keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferred'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /content/train"
          ]
        }
      ]
    }
  ]
}