{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Create a directory for the dataset\n",
        "data_dir = '/content/synthetic_dataset'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "num_classes = 2\n",
        "num_videos_per_class = 5\n",
        "video_length = 30  # Number of frames\n",
        "frame_height = 64\n",
        "frame_width = 64\n",
        "\n",
        "# Generate synthetic videos\n",
        "for class_id in range(num_classes):\n",
        "    class_dir = os.path.join(data_dir, f'class_{class_id}')\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for video_id in range(num_videos_per_class):\n",
        "        video_path = os.path.join(class_dir, f'video_{video_id}.avi')\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        out = cv2.VideoWriter(video_path, fourcc, 20.0, (frame_width, frame_height))\n",
        "\n",
        "        for _ in range(video_length):\n",
        "            # Create a random frame\n",
        "            frame = np.random.randint(0, 256, (frame_height, frame_width, 3), dtype=np.uint8)\n",
        "            out.write(frame)\n",
        "\n",
        "        out.release()\n",
        "\n",
        "print(\"Synthetic video dataset created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jmlOR-_16r9",
        "outputId": "2812e846-045c-4bb8-ac77-765370bda829"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic video dataset created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_videos(data_dir, img_size=(64, 64), max_frames=30):\n",
        "    X, y = [], []\n",
        "    class_names = os.listdir(data_dir)\n",
        "\n",
        "    for label, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for video_file in os.listdir(class_dir):\n",
        "            video_path = os.path.join(class_dir, video_file)\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "            frames = []\n",
        "            while len(frames) < max_frames:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, img_size)\n",
        "                frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                X.append(frames)\n",
        "                y.append(label)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "    return np.array(X), np.array(y), class_names\n",
        "\n",
        "# Load the synthetic dataset\n",
        "X, y, class_names = load_videos(data_dir)\n",
        "X = X / 255.0  # Normalize the data\n"
      ],
      "metadata": {
        "id": "vA8fN-XY2DL3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create Synthetic Video Dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '/content/synthetic_dataset'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "num_classes = 2\n",
        "num_videos_per_class = 5\n",
        "video_length = 30  # Number of frames\n",
        "frame_height = 64\n",
        "frame_width = 64\n",
        "\n",
        "for class_id in range(num_classes):\n",
        "    class_dir = os.path.join(data_dir, f'class_{class_id}')\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for video_id in range(num_videos_per_class):\n",
        "        video_path = os.path.join(class_dir, f'video_{video_id}.avi')\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        out = cv2.VideoWriter(video_path, fourcc, 20.0, (frame_width, frame_height))\n",
        "\n",
        "        for _ in range(video_length):\n",
        "            frame = np.random.randint(0, 256, (frame_height, frame_width, 3), dtype=np.uint8)\n",
        "            out.write(frame)\n",
        "\n",
        "        out.release()\n",
        "\n",
        "print(\"Synthetic video dataset created!\")\n",
        "\n",
        "# Step 2: Load the Synthetic Video Dataset\n",
        "def load_videos(data_dir, img_size=(64, 64), max_frames=30):\n",
        "    X, y = [], []\n",
        "    class_names = os.listdir(data_dir)\n",
        "\n",
        "    for label, class_name in enumerate(class_names):\n",
        "        class_dir = os.path.join(data_dir, class_name)\n",
        "        for video_file in os.listdir(class_dir):\n",
        "            video_path = os.path.join(class_dir, video_file)\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "            frames = []\n",
        "            while len(frames) < max_frames:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.resize(frame, img_size)\n",
        "                frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                X.append(frames)\n",
        "                y.append(label)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "    return np.array(X), np.array(y), class_names\n",
        "\n",
        "X, y, class_names = load_videos(data_dir)\n",
        "X = X / 255.0  # Normalize the data\n",
        "\n",
        "# Step 3: Split the Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Build the Model\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(30, 64, 64, 3)),\n",
        "    layers.MaxPooling3D((2, 2, 2)),\n",
        "    layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
        "    layers.MaxPooling3D((2, 2, 2)),\n",
        "    layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
        "    layers.MaxPooling3D((2, 2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.2f}')\n",
        "\n",
        "# Step 7: Save the Model\n",
        "model.save('video_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxwIu-tj2FlT",
        "outputId": "1f5b9db8-ffa6-472f-c142-6b7a7167fb5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic video dataset created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.5000 - loss: 0.6946 - val_accuracy: 0.5000 - val_loss: 1.0613\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 1.0320 - val_accuracy: 0.5000 - val_loss: 0.7864\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.7839 - val_accuracy: 0.5000 - val_loss: 0.7031\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6991 - val_accuracy: 0.5000 - val_loss: 0.6974\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.6950 - val_accuracy: 0.5000 - val_loss: 0.6934\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.5000 - val_loss: 0.6993\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.6952 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6894 - val_accuracy: 0.5000 - val_loss: 0.6969\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6900 - val_accuracy: 0.5000 - val_loss: 0.6934\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6854 - val_accuracy: 0.5000 - val_loss: 0.6956\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.5000 - loss: 0.6956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.50\n"
          ]
        }
      ]
    }
  ]
}